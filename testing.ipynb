{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-16T17:24:50.803262Z",
     "start_time": "2023-12-16T17:24:47.762692Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from opinionmining import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "['/Users/jossinger/Dropbox/Studies/Bath_Artificial_Intelligence/Course Material/6_NLP/Programming/Submission/data/Reviews-9-products/norton.txt',\n '/Users/jossinger/Dropbox/Studies/Bath_Artificial_Intelligence/Course Material/6_NLP/Programming/Submission/data/Reviews-9-products/Nokia 6600.txt',\n '/Users/jossinger/Dropbox/Studies/Bath_Artificial_Intelligence/Course Material/6_NLP/Programming/Submission/data/Reviews-9-products/Hitachi router.txt',\n '/Users/jossinger/Dropbox/Studies/Bath_Artificial_Intelligence/Course Material/6_NLP/Programming/Submission/data/Reviews-9-products/ipod.txt',\n '/Users/jossinger/Dropbox/Studies/Bath_Artificial_Intelligence/Course Material/6_NLP/Programming/Submission/data/Reviews-9-products/Diaper Champ.txt',\n '/Users/jossinger/Dropbox/Studies/Bath_Artificial_Intelligence/Course Material/6_NLP/Programming/Submission/data/Reviews-9-products/Linksys Router.txt',\n '/Users/jossinger/Dropbox/Studies/Bath_Artificial_Intelligence/Course Material/6_NLP/Programming/Submission/data/Reviews-9-products/Canon S100.txt',\n '/Users/jossinger/Dropbox/Studies/Bath_Artificial_Intelligence/Course Material/6_NLP/Programming/Submission/data/Reviews-9-products/MicroMP3.txt',\n '/Users/jossinger/Dropbox/Studies/Bath_Artificial_Intelligence/Course Material/6_NLP/Programming/Submission/data/Reviews-9-products/Canon PowerShot SD500.txt',\n '/Users/jossinger/Dropbox/Studies/Bath_Artificial_Intelligence/Course Material/6_NLP/Programming/Submission/data/CustomerReviews-3_domains/Computer.txt',\n '/Users/jossinger/Dropbox/Studies/Bath_Artificial_Intelligence/Course Material/6_NLP/Programming/Submission/data/CustomerReviews-3_domains/Speaker.txt',\n '/Users/jossinger/Dropbox/Studies/Bath_Artificial_Intelligence/Course Material/6_NLP/Programming/Submission/data/CustomerReviews-3_domains/Router.txt',\n '/Users/jossinger/Dropbox/Studies/Bath_Artificial_Intelligence/Course Material/6_NLP/Programming/Submission/data/Customer_review_data/Nokia 6610.txt',\n '/Users/jossinger/Dropbox/Studies/Bath_Artificial_Intelligence/Course Material/6_NLP/Programming/Submission/data/Customer_review_data/Canon G3.txt',\n '/Users/jossinger/Dropbox/Studies/Bath_Artificial_Intelligence/Course Material/6_NLP/Programming/Submission/data/Customer_review_data/Creative Labs Nomad Jukebox Zen Xtra 40GB.txt',\n '/Users/jossinger/Dropbox/Studies/Bath_Artificial_Intelligence/Course Material/6_NLP/Programming/Submission/data/Customer_review_data/Apex AD2600 Progressive-scan DVD player.txt',\n '/Users/jossinger/Dropbox/Studies/Bath_Artificial_Intelligence/Course Material/6_NLP/Programming/Submission/data/Customer_review_data/Nikon coolpix 4300.txt']"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths = []\n",
    "for folder in os.listdir(full_path):\n",
    "    try:\n",
    "        for file in os.listdir(os.path.join(full_path, folder)):\n",
    "            if str(file) != \"Readme.txt\" and str(file) != \".DS_Store\":\n",
    "                paths.append(os.path.join(full_path, folder, file))\n",
    "    except NotADirectoryError:\n",
    "        continue\n",
    "paths"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-16T17:24:50.824477Z",
     "start_time": "2023-12-16T17:24:50.807501Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class part_of_speech:\n",
    "\n",
    "    @classmethod\n",
    "    def tag_sentence(cls, sentence):\n",
    "        doc:Doc = nlp(sentence)\n",
    "        return [(token.pos_, token.dep_, token.lemma_) for token in doc]\n",
    "    \n",
    "    \n",
    "    @classmethod\n",
    "    def noun_phrases(cls, sentence):\n",
    "        doc: Doc = nlp(sentence)\n",
    "        \n",
    "        return list([token for token in doc if token.tag_ == \"NOUN\"])   "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-16T17:24:50.824873Z",
     "start_time": "2023-12-16T17:24:50.821009Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "SAVE = False\n",
    "pwd = os.getcwd()\n",
    "filename = \"db.csv\"\n",
    "if SAVE:\n",
    "    db = ReviewDatabase(paths).dataframe.to_csv(os.path.join(pwd, filename))\n",
    "else:\n",
    "    db = pd.read_csv(filename)\n",
    "\n",
    "database = db.head()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-16T18:07:11.826544Z",
     "start_time": "2023-12-16T18:07:11.806429Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'spacy.tokens.span.Span' object has no attribute 'ent_iob_'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[130], line 19\u001B[0m\n\u001B[1;32m     16\u001B[0m     \u001B[38;5;28mprint\u001B[39m(noun_chunks)\n\u001B[1;32m     18\u001B[0m processed_database \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame()\n\u001B[0;32m---> 19\u001B[0m processed_database[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNounPhrases\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mSeries\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mmap\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mextract_np\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdatabase\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mSentence\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/nlp_env/lib/python3.9/site-packages/pandas/core/series.py:493\u001B[0m, in \u001B[0;36mSeries.__init__\u001B[0;34m(self, data, index, dtype, name, copy, fastpath)\u001B[0m\n\u001B[1;32m    491\u001B[0m     \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[1;32m    492\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 493\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[43mcom\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmaybe_iterable_to_list\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    494\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m is_list_like(data) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(data) \u001B[38;5;129;01mand\u001B[39;00m dtype \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    495\u001B[0m         \u001B[38;5;66;03m# GH 29405: Pre-2.0, this defaulted to float.\u001B[39;00m\n\u001B[1;32m    496\u001B[0m         dtype \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mdtype(\u001B[38;5;28mobject\u001B[39m)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/nlp_env/lib/python3.9/site-packages/pandas/core/common.py:301\u001B[0m, in \u001B[0;36mmaybe_iterable_to_list\u001B[0;34m(obj)\u001B[0m\n\u001B[1;32m    297\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    298\u001B[0m \u001B[38;5;124;03mIf obj is Iterable but not list-like, consume into list.\u001B[39;00m\n\u001B[1;32m    299\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    300\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(obj, abc\u001B[38;5;241m.\u001B[39mIterable) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(obj, abc\u001B[38;5;241m.\u001B[39mSized):\n\u001B[0;32m--> 301\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    302\u001B[0m obj \u001B[38;5;241m=\u001B[39m cast(Collection, obj)\n\u001B[1;32m    303\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m obj\n",
      "Cell \u001B[0;32mIn[130], line 19\u001B[0m, in \u001B[0;36m<lambda>\u001B[0;34m(x)\u001B[0m\n\u001B[1;32m     16\u001B[0m     \u001B[38;5;28mprint\u001B[39m(noun_chunks)\n\u001B[1;32m     18\u001B[0m processed_database \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame()\n\u001B[0;32m---> 19\u001B[0m processed_database[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNounPhrases\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mSeries(\u001B[38;5;28mmap\u001B[39m(\u001B[38;5;28;01mlambda\u001B[39;00m x: \u001B[43mextract_np\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m, database\u001B[38;5;241m.\u001B[39mSentence))\n",
      "Cell \u001B[0;32mIn[130], line 14\u001B[0m, in \u001B[0;36mextract_np\u001B[0;34m(string)\u001B[0m\n\u001B[1;32m     12\u001B[0m noun_chunks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(doc\u001B[38;5;241m.\u001B[39mnoun_chunks)\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m token \u001B[38;5;129;01min\u001B[39;00m noun_chunks:\n\u001B[0;32m---> 14\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtoken:\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtoken\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m  ent:\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtoken\u001B[38;5;241m.\u001B[39ment_iob_\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28mprint\u001B[39m(noun_chunks)\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'spacy.tokens.span.Span' object has no attribute 'ent_iob_'"
     ]
    }
   ],
   "source": [
    "from spacy.symbols import *\n",
    "from spacy import displacy\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer\n",
    "\n",
    "def is_stop(word):\n",
    "    \n",
    "    return \n",
    "def extract_np(string):\n",
    "    doc = nlp(string)\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    ners = [str(ents) for ents in doc.ents if str(ents.label_) not in set([\"MONEY\", \"DATE\", \"TIME\", \"QUANTITY\", \"CARDINAL\", \"PERCENT\"])]\n",
    "    noun_chunks = list(doc.noun_chunks)\n",
    "    for token in noun_chunks:\n",
    "        print(f\"token:{token}  ent:{token.ent_iob_}\")\n",
    "        \n",
    "    print(noun_chunks)\n",
    "\n",
    "processed_database = pd.DataFrame()\n",
    "processed_database[\"NounPhrases\"] = pd.Series(map(lambda x: extract_np(x), database.Sentence))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-16T21:56:16.998629Z",
     "start_time": "2023-12-16T21:56:16.892596Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "    What is the product?\n",
    "What are the opinion categories?\n",
    "What is the sentiment of the opinion categories?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "    processed_tokens = []\n",
    "    # for token in noun_chunks:\n",
    "    #     if any(te in ners for te in token.ents):\n",
    "    #         print(f\"{token}\")\n",
    "    #     print(\"/n\")\n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "def name_entity_step_over(string):\n",
    "    doc = nlp(string)\n",
    "    \n",
    "\n",
    "    \n",
    "    # print(noun_chunks)    \n",
    "    # for np in noun_chunks:\n",
    "    #     words = []\n",
    "    #     for w in np:\n",
    "    #         print(w)\n",
    "    #         if w.is_stop:\n",
    "    #             continue\n",
    "    #         print(w.ent_id)\n",
    "# def is_prohibited(token):\n",
    "#     return any(token.isent)\n",
    "            \n",
    "    # removed_stopwords = [\" \".join([str(w) for w in np if not w.is_stop]).strip() for np in doc.noun_chunks]\n",
    "    # print(removed_stopwords)\n",
    "    # stemmed_np = [stemmer.stem(np) for np in removed_stopwords if np is not None]\n",
    "    # return stemmed_np\n",
    "\n",
    "# def subtree(sentence):\n",
    "#     flags= set([nsubj]) \n",
    "#     doc = nlp(sentence)\n",
    "#     df = pd.DataFrame()\n",
    "#     for t in doc:\n",
    "#         pos_series = pd.Series({\"token\":t,\n",
    "#                             \"pos\":t.pos_,\n",
    "#                             \"lemma\":t.lemma_,\n",
    "#                             \"dep\":t.dep_,\n",
    "#                             \"subtree\":list(t.subtree)\n",
    "#                             })\n",
    "#         df = pd.concat([df, pos_series.to_frame().T])\n",
    "#     return df\n",
    "# \n",
    "# subtree(database.Sentence[0])\n",
    "#     \n",
    "\n",
    "\n",
    "\n",
    "# processed_database"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
