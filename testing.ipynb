{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-17T14:41:58.767763Z",
     "start_time": "2023-12-17T14:41:56.772895Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from opinionmining import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "['/Users/jossinger/Dropbox/Studies/Bath_Artificial_Intelligence/Course Material/6_NLP/Programming/Submission/data/Reviews-9-products/norton.txt',\n '/Users/jossinger/Dropbox/Studies/Bath_Artificial_Intelligence/Course Material/6_NLP/Programming/Submission/data/Reviews-9-products/Nokia 6600.txt',\n '/Users/jossinger/Dropbox/Studies/Bath_Artificial_Intelligence/Course Material/6_NLP/Programming/Submission/data/Reviews-9-products/Hitachi router.txt',\n '/Users/jossinger/Dropbox/Studies/Bath_Artificial_Intelligence/Course Material/6_NLP/Programming/Submission/data/Reviews-9-products/ipod.txt',\n '/Users/jossinger/Dropbox/Studies/Bath_Artificial_Intelligence/Course Material/6_NLP/Programming/Submission/data/Reviews-9-products/Diaper Champ.txt',\n '/Users/jossinger/Dropbox/Studies/Bath_Artificial_Intelligence/Course Material/6_NLP/Programming/Submission/data/Reviews-9-products/Linksys Router.txt',\n '/Users/jossinger/Dropbox/Studies/Bath_Artificial_Intelligence/Course Material/6_NLP/Programming/Submission/data/Reviews-9-products/Canon S100.txt',\n '/Users/jossinger/Dropbox/Studies/Bath_Artificial_Intelligence/Course Material/6_NLP/Programming/Submission/data/Reviews-9-products/MicroMP3.txt',\n '/Users/jossinger/Dropbox/Studies/Bath_Artificial_Intelligence/Course Material/6_NLP/Programming/Submission/data/Reviews-9-products/Canon PowerShot SD500.txt',\n '/Users/jossinger/Dropbox/Studies/Bath_Artificial_Intelligence/Course Material/6_NLP/Programming/Submission/data/CustomerReviews-3_domains/Computer.txt',\n '/Users/jossinger/Dropbox/Studies/Bath_Artificial_Intelligence/Course Material/6_NLP/Programming/Submission/data/CustomerReviews-3_domains/Speaker.txt',\n '/Users/jossinger/Dropbox/Studies/Bath_Artificial_Intelligence/Course Material/6_NLP/Programming/Submission/data/CustomerReviews-3_domains/Router.txt',\n '/Users/jossinger/Dropbox/Studies/Bath_Artificial_Intelligence/Course Material/6_NLP/Programming/Submission/data/Customer_review_data/Nokia 6610.txt',\n '/Users/jossinger/Dropbox/Studies/Bath_Artificial_Intelligence/Course Material/6_NLP/Programming/Submission/data/Customer_review_data/Canon G3.txt',\n '/Users/jossinger/Dropbox/Studies/Bath_Artificial_Intelligence/Course Material/6_NLP/Programming/Submission/data/Customer_review_data/Creative Labs Nomad Jukebox Zen Xtra 40GB.txt',\n '/Users/jossinger/Dropbox/Studies/Bath_Artificial_Intelligence/Course Material/6_NLP/Programming/Submission/data/Customer_review_data/Apex AD2600 Progressive-scan DVD player.txt',\n '/Users/jossinger/Dropbox/Studies/Bath_Artificial_Intelligence/Course Material/6_NLP/Programming/Submission/data/Customer_review_data/Nikon coolpix 4300.txt']"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths = []\n",
    "for folder in os.listdir(full_path):\n",
    "    try:\n",
    "        for file in os.listdir(os.path.join(full_path, folder)):\n",
    "            if str(file) != \"Readme.txt\" and str(file) != \".DS_Store\":\n",
    "                paths.append(os.path.join(full_path, folder, file))\n",
    "    except NotADirectoryError:\n",
    "        continue\n",
    "paths"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T14:41:58.779690Z",
     "start_time": "2023-12-17T14:41:58.769392Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class part_of_speech:\n",
    "\n",
    "    @classmethod\n",
    "    def tag_sentence(cls, sentence):\n",
    "        doc:Doc = nlp(sentence)\n",
    "        return [(token.pos_, token.dep_, token.lemma_) for token in doc]\n",
    "    \n",
    "    \n",
    "    @classmethod\n",
    "    def noun_phrases(cls, sentence):\n",
    "        doc: Doc = nlp(sentence)\n",
    "        \n",
    "        return list([token for token in doc if token.tag_ == \"NOUN\"])   "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T14:41:58.792654Z",
     "start_time": "2023-12-17T14:41:58.781834Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "SAVE = False\n",
    "pwd = os.getcwd()\n",
    "filename = \"db.csv\"\n",
    "if SAVE:\n",
    "    db = ReviewDatabase(paths).dataframe.to_csv(os.path.join(pwd, filename))\n",
    "else:\n",
    "    db = pd.read_csv(filename)\n",
    "\n",
    "database = db.head()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T14:41:58.816789Z",
     "start_time": "2023-12-17T14:41:58.786148Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from spacy.symbols import *\n",
    "from spacy import displacy\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T14:41:59.583748Z",
     "start_time": "2023-12-17T14:41:58.806231Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NAV, installation instructions, NIS, installation, result, junk software]\n",
      "[type, software]\n",
      "[Norton, products, installation]\n",
      "[McAfee Anti-Virus 8, computer, owner, NOT_problems]\n",
      "[Norton, past years, years, software]\n"
     ]
    },
    {
     "data": {
      "text/plain": "0    [NAV, installation instructions, NIS, installa...\n1                                   [, type, software]\n2                     [Norton, products, installation]\n3    [McAfee Anti-Virus 8, computer, owner, NOT_pro...\n4                [Norton, past years, years, software]\nName: NounPhrases, dtype: object"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "processed_database = pd.DataFrame()\n",
    "def extract_categories(string):\n",
    "    doc = nlp(string)\n",
    "    prohibited_ents = set([\"MONEY\", \"DATE\", \"TIME\", \"QUANTITY\", \"CARDINAL\", \"PERCENT\"])\n",
    "    named_entities = [ents for ents in doc.ents if str(ents.label_) not in prohibited_ents]\n",
    "    named_entities_set = set([token for word in named_entities for token in word if not token.is_punct and not token.is_digit])\n",
    "    np = [\" \".join([str(ent) for ent in doc.ents if str(ent.label_) not in prohibited_ents])]\n",
    "    for chunk in doc.noun_chunks:\n",
    "        chunk_ = []\n",
    "        for i, token in enumerate(chunk):\n",
    "            if token in named_entities_set or token.is_punct or token.is_digit or token.is_stop:\n",
    "                continue\n",
    "            if i >= 1:\n",
    "                if str(chunk[i-1]) == \"no\":\n",
    "                    chunk_.append(f\"NOT_{str(token)}\")\n",
    "                    continue\n",
    "            chunk_.append(str(token))\n",
    "        if len(chunk_)!= 0:\n",
    "            np.append(\" \".join(chunk_))\n",
    "    print([nlp(t) for t in np if t != \"\"])\n",
    "    return np\n",
    "    \n",
    "processed_database[\"NounPhrases\"] = pd.Series(map(lambda x: extract_categories(x), database.Sentence))\n",
    "processed_database[\"NounPhrases\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T14:53:33.134242Z",
     "start_time": "2023-12-17T14:53:33.000139Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "What is the product?\n",
    "What are the opinion categories?\n",
    "What is the sentiment of the opinion categories?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "      token    pos     lemma     dep                             children\n0       Why  SCONJ       why  advmod                                   []\n0        is    AUX        be    ROOT                [Why, it, install, ?]\n0        it   PRON        it   nsubj                                   []\n0      that  SCONJ      that    mark                                   []\n0         I   PRON         I   nsubj                                   []\n0       can    AUX       can     aux                                   []\n0   install   VERB   install   ccomp  [that, I, can, type, and, installs]\n0       any    DET       any     det                                   []\n0     other    ADJ     other    amod                                   []\n0      type   NOUN      type    dobj                     [any, other, of]\n0        of    ADP        of    prep                           [software]\n0  software   NOUN  software    pobj                                   []\n0       and  CCONJ       and      cc                                   []\n0        it   PRON        it   nsubj                                   []\n0  installs   VERB   install    conj                     [it, and, works]\n0       and  CCONJ       and      cc                                   []\n0     works   VERB      work    conj                           [properly]\n0  properly    ADV  properly  advmod                                   []\n0         ?  PUNCT         ?   punct                                   []",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>token</th>\n      <th>pos</th>\n      <th>lemma</th>\n      <th>dep</th>\n      <th>children</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Why</td>\n      <td>SCONJ</td>\n      <td>why</td>\n      <td>advmod</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>is</td>\n      <td>AUX</td>\n      <td>be</td>\n      <td>ROOT</td>\n      <td>[Why, it, install, ?]</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>it</td>\n      <td>PRON</td>\n      <td>it</td>\n      <td>nsubj</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>that</td>\n      <td>SCONJ</td>\n      <td>that</td>\n      <td>mark</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>I</td>\n      <td>PRON</td>\n      <td>I</td>\n      <td>nsubj</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>can</td>\n      <td>AUX</td>\n      <td>can</td>\n      <td>aux</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>install</td>\n      <td>VERB</td>\n      <td>install</td>\n      <td>ccomp</td>\n      <td>[that, I, can, type, and, installs]</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>any</td>\n      <td>DET</td>\n      <td>any</td>\n      <td>det</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>other</td>\n      <td>ADJ</td>\n      <td>other</td>\n      <td>amod</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>type</td>\n      <td>NOUN</td>\n      <td>type</td>\n      <td>dobj</td>\n      <td>[any, other, of]</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>of</td>\n      <td>ADP</td>\n      <td>of</td>\n      <td>prep</td>\n      <td>[software]</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>software</td>\n      <td>NOUN</td>\n      <td>software</td>\n      <td>pobj</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>and</td>\n      <td>CCONJ</td>\n      <td>and</td>\n      <td>cc</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>it</td>\n      <td>PRON</td>\n      <td>it</td>\n      <td>nsubj</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>installs</td>\n      <td>VERB</td>\n      <td>install</td>\n      <td>conj</td>\n      <td>[it, and, works]</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>and</td>\n      <td>CCONJ</td>\n      <td>and</td>\n      <td>cc</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>works</td>\n      <td>VERB</td>\n      <td>work</td>\n      <td>conj</td>\n      <td>[properly]</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>properly</td>\n      <td>ADV</td>\n      <td>properly</td>\n      <td>advmod</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>?</td>\n      <td>PUNCT</td>\n      <td>?</td>\n      <td>punct</td>\n      <td>[]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def subtree(sentence):\n",
    "    flags= set([nsubj]) \n",
    "    doc = nlp(sentence)\n",
    "    df = pd.DataFrame()\n",
    "    for t in doc:\n",
    "        pos_series = pd.Series({\"token\":t,\n",
    "                            \"pos\":t.pos_,\n",
    "                            \"lemma\":t.lemma_,\n",
    "                            \"dep\":t.dep_,\n",
    "                            \"children\":[child for child in t.children]\n",
    "                            })\n",
    "        df = pd.concat([df, pos_series.to_frame().T])\n",
    "    return df\n",
    "\n",
    "subtree(database.Sentence[1])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T14:41:59.678097Z",
     "start_time": "2023-12-17T14:41:59.641464Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (866095495.py, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;36m  Cell \u001B[0;32mIn[8], line 11\u001B[0;36m\u001B[0m\n\u001B[0;31m    doc = nlp(string)\u001B[0m\n\u001B[0m    ^\u001B[0m\n\u001B[0;31mIndentationError\u001B[0m\u001B[0;31m:\u001B[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "    processed_tokens = []\n",
    "    # for token in noun_chunks:\n",
    "    #     if any(te in ners for te in token.ents):\n",
    "    #         print(f\"{token}\")\n",
    "    #     print(\"/n\")\n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "def name_entity_step_over(string):\n",
    "    doc = nlp(string)\n",
    "    \n",
    "\n",
    "    \n",
    "    # print(noun_chunks)    \n",
    "    # for np in noun_chunks:\n",
    "    #     words = []\n",
    "    #     for w in np:\n",
    "    #         print(w)\n",
    "    #         if w.is_stop:\n",
    "    #             continue\n",
    "    #         print(w.ent_id)\n",
    "# def is_prohibited(token):\n",
    "#     return any(token.isent)\n",
    "            \n",
    "    # removed_stopwords = [\" \".join([str(w) for w in np if not w.is_stop]).strip() for np in doc.noun_chunks]\n",
    "    # print(removed_stopwords)\n",
    "    # stemmed_np = [stemmer.stem(np) for np in removed_stopwords if np is not None]\n",
    "    # return stemmed_np\n",
    "\n",
    "# def subtree(sentence):\n",
    "#     flags= set([nsubj]) \n",
    "#     doc = nlp(sentence)\n",
    "#     df = pd.DataFrame()\n",
    "#     for t in doc:\n",
    "#         pos_series = pd.Series({\"token\":t,\n",
    "#                             \"pos\":t.pos_,\n",
    "#                             \"lemma\":t.lemma_,\n",
    "#                             \"dep\":t.dep_,\n",
    "#                             \"subtree\":list(t.subtree)\n",
    "#                             })\n",
    "#         df = pd.concat([df, pos_series.to_frame().T])\n",
    "#     return df\n",
    "# \n",
    "# subtree(database.Sentence[0])\n",
    "#     \n",
    "\n",
    "\n",
    "\n",
    "# processed_database"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T14:41:59.687914Z",
     "start_time": "2023-12-17T14:41:59.675364Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-17T14:41:59.682129Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-17T14:41:59.683993Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
