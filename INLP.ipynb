{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from opinionmining import *\n",
    "from typing import Set\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "import itertools\n",
    "from textblob import TextBlob\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import LeaveOneOut, cross_val_predict\n",
    "# Set save variable\n",
    "SAVE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction_table(db:pd.DataFrame, filename)->pd.DataFrame:\n",
    "    # Extract nouns from the database sentences\n",
    "   \n",
    "   \n",
    "    database = db.copy()\n",
    "    database[\"ExtractedNouns\"] = database[\"Sentence\"].apply(lambda x : FeatureExtraction.categories(x))\n",
    "    print(f\"Nouns extracted\")\n",
    "    #Create a copy of the database to perform feature extraction\n",
    "    category_table = database.loc[:,\"Product_ID\":\"Sentence_ID\"].copy()\n",
    "    \n",
    "    \n",
    "    # Perform Stemming on the sentences\n",
    "    category_table[\"Sentence\"] = database.Sentence.apply(lambda x: FeatureExtraction.add_negations(x))\n",
    "    category_table[\"Stemmed_Sentence\"] = category_table.Sentence.apply(lambda x: FeatureExtraction.stemming([x])[0])\n",
    "    print(\"Performed Stemming\")\n",
    "    # Remove the stop words\n",
    "    category_table[\"Clean_Sentence\"] = category_table[\"Stemmed_Sentence\"].apply(lambda x: FeatureExtraction.remove_stop([x])[0])\n",
    "    print(\"Stopwords removed\")\n",
    "    \n",
    "    # Perform fuzzymatching on the least frequently occuring nouns byspliting according to frequency\n",
    "    flattened_nouns = [item for sublist in database.ExtractedNouns for item in sublist]\n",
    "    frequency_sorted_nouns = [item for item, count in Counter(flattened_nouns).most_common()]\n",
    "    midpoint = len(frequency_sorted_nouns)//2\n",
    "    firsthalf = frequency_sorted_nouns[:midpoint]\n",
    "    secondhalf = frequency_sorted_nouns[midpoint:] \n",
    "    \n",
    "    # Perform fuzzymatching\n",
    "    D = FeatureExtraction.fuzzy_match_categories(test_categories=secondhalf, target_categories=firsthalf)\n",
    "    transactions = list(map(lambda x: FeatureExtraction.fuzzy_match_categories(x, D), database.ExtractedNouns))\n",
    "    stemmed_transactions = list(map(lambda x: FeatureExtraction.stemming(x), transactions))\n",
    "    category_table[\"Stemmed_Transactions\"] = stemmed_transactions\n",
    "    item_set = [lst for lst in stemmed_transactions if len(lst) != 0]\n",
    "    category_table.head()\n",
    "    print(f\"fuzzy matching done\")\n",
    "    \n",
    "    # Create a dictionary that remaps stemmed words to most commonly occuring forms of that word\n",
    "    stemming_tuples = []\n",
    "    stemming_dict = {}\n",
    "    for i, l in enumerate(transactions):\n",
    "        lst = []\n",
    "        for j, word in enumerate(l):\n",
    "            original_word = word\n",
    "            stemmed_word = stemmed_transactions[i][j]\n",
    "            stemming_tuples.append((stemmed_word, original_word))\n",
    "    stemming_tuples = sorted(stemming_tuples, key= lambda x: x[0])\n",
    "    \n",
    "    # create a key:list pair of all the spellings associated to a stemmed word\n",
    "    key_valuelists = {k : [] for k, v in stemming_tuples}\n",
    "    for (key, value) in stemming_tuples:\n",
    "        try:\n",
    "            current_list = key_valuelists.get(key)\n",
    "            current_list.append(value)\n",
    "            key_valuelists[key] = current_list\n",
    "        except KeyError as e:\n",
    "            print(e)\n",
    "    \n",
    "    # create the dictionary of the most commonly occuring spelling in each spelling list\n",
    "    stemming_dict = {k: Counter(v).most_common(1)[0][0] for k, v in key_valuelists.items()}\n",
    "    print(\"Stemming Dictionary created\")\n",
    "    \n",
    "# FREQUENT FEATURE SELECTION via apriori algorithm to determine the most frequent features\n",
    "# __________________________________________________________________________________________________________________\n",
    "    d = item_set\n",
    "    te = TransactionEncoder()\n",
    "    te_ary = te.fit(d).transform(d)\n",
    "    df = pd.DataFrame(te_ary, columns = te.columns_)\n",
    "    frequent_items = apriori(df, min_support=0.01, use_colnames=True) #apriori min support set to 1%\n",
    "    print(\"Frequent Features extracted\")\n",
    "    \n",
    "# FEATURE PRUNING: pruning on sets of compound words where there is a minimum distance between each set of nouns\n",
    "# __________________________________________________________________________________________________________________\n",
    "    def compact_distance_between_consecutive_numbers_is_valid(lst:List[int], max_dist=3)->bool:\n",
    "        # for each permutation calculate the distance between the words, if any of the numbers are larger than 3, continue\n",
    "        for i in range(len(lst)-1):\n",
    "            if lst[i+1] - lst[i] > max_dist:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def test_compact_phrase(phrase, sentences):\n",
    "        # test whether a given phrase is within 2 words of its partner. This is time complexity O(n!), acceptable because max n=3.\n",
    "        set_words = set(phrase)\n",
    "        compact_phrase_count = 0\n",
    "        for sentence in sentences:\n",
    "            if compact_phrase_count >= 2:\n",
    "                return True\n",
    "            sentence_str = sentence.split(\" \")\n",
    "            set_sentence = set(sentence_str)\n",
    "            if set_words.issubset(set_sentence):\n",
    "                indicies = [sentence_str.index(word) for word in set_words]\n",
    "                is_compact = compact_distance_between_consecutive_numbers_is_valid(indicies)\n",
    "                if is_compact:\n",
    "                    compact_phrase_count += 1\n",
    "            else:\n",
    "                continue\n",
    "        return False\n",
    "\n",
    "    test_sents = list(category_table[\"Stemmed_Sentence\"])\n",
    "    phrases = [phrs for phrs in frequent_items[\"itemsets\"] if len(phrs) >= 2 and len(phrs) < 4]\n",
    "    single_phrases = [phrs for phrs in frequent_items[\"itemsets\"] if len(phrs) < 2]\n",
    "    compact_phrases = [ph for ph in phrases if test_compact_phrase(ph, test_sents)]\n",
    "    print(\"performed feature pruning on compound phrases\")\n",
    "# PSUPPORT PRUNING: pruning on single words where there the word occurs at least twice outside of a frequent feature \n",
    "# __________________________________________________________________________________________________________________\n",
    "    \n",
    "    def get_supersets(single_phrase:set, compact_phrases:List[set])->List[set]:\n",
    "        return [phrase for phrase in compact_phrases if single_phrase.issubset(compact_phrases)]\n",
    "\n",
    "    def is_valid_noun(ftr, nouns):\n",
    "        for noun in nouns:\n",
    "            if ftr == noun.text:\n",
    "                return True\n",
    "        return False\n",
    "                \n",
    "\n",
    "    def count_sentence(ftr, sentence, feature_phrases):\n",
    "        # count the number of times ftr appears as a noun \n",
    "        # where the sentence does not contain a feature phrase that also contains ftr\n",
    "        sentence_str = sentence.split(\" \")\n",
    "        sentence_set = set(sentence_str)\n",
    "        if ftr.issubset(sentence_set):\n",
    "            doc = nlp(sentence)\n",
    "            nouns = [token for token in doc if token.pos_ in [\"NOUN\", \"PROPN\"]]\n",
    "            # check if the noun list contains a feature phrase with the same ftr\n",
    "            super_sets = get_supersets(ftr, compact_phrases=feature_phrases)\n",
    "            if any(super for super in super_sets if ftr.issubset(super)) and not is_valid_noun(ftr, nouns):\n",
    "                return 0\n",
    "            else:\n",
    "                return 1\n",
    "        return 0\n",
    "\n",
    "    def p_support_pruning(single_phrases:List[set], sentences:List[set], compact_phrases:List[set], threshold=3):\n",
    "        p_support = [0] * len(single_phrases)\n",
    "        for i, ftr in enumerate(single_phrases):\n",
    "            for sentence in sentences:\n",
    "                p_support[i] += count_sentence(ftr, sentence, compact_phrases)\n",
    "        return [phrase for i, phrase in enumerate(single_phrases) if p_support[i] > threshold]\n",
    "            \n",
    "    reduced_single_phrases = p_support_pruning( single_phrases=single_phrases, \n",
    "                                                sentences=test_sents, \n",
    "                                                compact_phrases=compact_phrases)\n",
    "    combined_features = reduced_single_phrases + compact_phrases\n",
    "    print(\"Performed psupport pruning\")\n",
    "# OPINION WORDS AND EFFECTIVE OPINIONS: pruning on single words where there the word occurs at least twice outside of a frequent feature \n",
    "# __________________________________________________________________________________________________________________\n",
    "   \n",
    "#    METHODOLOGY 1: Search the surrounding adjectives within some distance = 3\n",
    "    effective_opinion_sets = []\n",
    "\n",
    "    features = combined_features\n",
    "    sentences = category_table.Sentence.apply(lambda x: FeatureExtraction.clean_sentence(x))\n",
    "    opinion_words = []\n",
    "    feature_set = [word for feature in features for word in feature]\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        try:\n",
    "            doc = nlp(sentence)\n",
    "            sentence_set = set(token.text for token in doc)\n",
    "            sentence_str = [token.text for token in doc]\n",
    "            op_sets = []\n",
    "            op_words = []\n",
    "            for feature in features:\n",
    "                if feature.issubset(sentence_set):\n",
    "                    feature_position = sentence_str.index(list(feature)[0])    \n",
    "                    adjectives = [(feature_position, feature, i, token) for i, token in enumerate(doc) if token.pos_ == \"ADJ\"]\n",
    "                    all_adjectives = [token.text for token in doc if token.pos_ == \"ADJ\"]\n",
    "                    valid_adjectives = [word for word in all_adjectives if not word in feature_set]\n",
    "                    op_words.append(valid_adjectives)\n",
    "                    op_sets.append(adjectives)\n",
    "            effective_opinion_sets.append(set([tup for op in op_sets for tup in op if len(op) > 0]))\n",
    "            opinion_words.append(set([w for op in op_words for w in op if len(op) > 0]))\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing sentence {i}: {e}\")\n",
    "\n",
    "    category_table[\"Opinion_Words\"] = opinion_words\n",
    "\n",
    "\n",
    "\n",
    "    def get_closest_adjective(ary_sets: Set[Tuple[int, frozenset, int, str]]) -> Set[Tuple[int, frozenset, int, str]]:\n",
    "        if len(ary_sets) == 0:\n",
    "            return set()\n",
    "        # initialize a dictionary\n",
    "        closest_sets = {}\n",
    "        for feature_index, feature, adj_index, adj in ary_sets:\n",
    "            if feature not in closest_sets:\n",
    "                closest_sets[feature] = (float('inf'), None) \n",
    "            # clculate distance between feature and adjective in sentence\n",
    "            distance = abs(feature_index - adj_index)\n",
    "            if distance < closest_sets[feature][0]:\n",
    "                closest_sets[feature] = (distance, (feature_index, feature, adj_index, adj))\n",
    "        # return the closest adjective n_ary from the dictionary\n",
    "        return set(value[1] for value in closest_sets.values() if value[1])\n",
    "    \n",
    "    # METHODOLOGY 2: only choose the adjectives that are connected to the nouns via amod dependancy\n",
    "    def get_opinion_words(sentences, frequent_features):\n",
    "        feature_set = set().union(*frequent_features)\n",
    "        stemmer = SnowballStemmer(\"english\")\n",
    "        opinion_words = []\n",
    "        effective_opinions = []\n",
    "\n",
    "        for s in sentences:\n",
    "            doc = nlp(s.lower())\n",
    "            adjectives = set([token.text for token in doc if token.pos_ == \"ADJ\"])\n",
    "            sentence_str = [word.text for word in doc]\n",
    "            sentence_set = set(sentence_str)\n",
    "            sentence_effective_adjectives = set()\n",
    "            # for each feature extract the adjective that modifies the feature\n",
    "            for ftr in frequent_features:\n",
    "                if ftr.issubset(sentence_set):\n",
    "                    indexes = [sentence_str.index(f) for f in ftr]\n",
    "                    # find the token at the index and search its children for the adjective modifier\n",
    "                    for index in indexes:\n",
    "                        for child in doc[index].children:\n",
    "                            if child.dep_ == \"amod\" and stemmer.stem(child.text) not in feature_set:\n",
    "                                sentence_effective_adjectives.add(child.text.replace(\"_\", \" \"))\n",
    "\n",
    "            valid_adjectives = set([stemmer.stem(word) for word in adjectives if stemmer.stem(word) not in feature_set])\n",
    "            \n",
    "\n",
    "            opinion_words.append(valid_adjectives)\n",
    "            effective_opinions.append(sentence_effective_adjectives)\n",
    "\n",
    "        return opinion_words, effective_opinions\n",
    "\n",
    "# INFREQUENT OPINION SELECTION\n",
    "# ___________________________________________________________________________________________________________________________\n",
    "    infreq_opinions = []\n",
    "    for i, opinion_set in enumerate(effective_opinion_sets):\n",
    "        s = [t for t in nlp(sentences.to_list()[i])]\n",
    "        closest_opinion_set = get_closest_adjective(opinion_set)\n",
    "        op_set = set()\n",
    "        if closest_opinion_set != set():\n",
    "            for _, feature, i_adj, adj in closest_opinion_set:\n",
    "                token = s[i_adj]\n",
    "                if token.text == adj.text:\n",
    "                    if token.head.text in feature:\n",
    "                        continue\n",
    "                    if token.head.pos_ == \"NOUN\":\n",
    "                        op_set.add((adj.text, token.head.text))\n",
    "        infreq_opinions.append(op_set)\n",
    "    category_table[\"Infrequent_Opinions\"] = infreq_opinions\n",
    "    \n",
    "# FEATURE TAG EXPANSION: expand tags so that they are legible. Do this by finding the most commonly occuring branch of each stem.\n",
    "# ___________________________________________________________________________________________________________________________\n",
    "    def phrase_voting(phrases, sentences):\n",
    "        \"\"\"\n",
    "        Perform phrase voting on a given set of phrases that \n",
    "        returns the most commonly occuring permutation of a phrase in a set of setnences\n",
    "        \"\"\"\n",
    "        phrase_dict = {k:list(itertools.permutations(k, len(k))) for k in phrases}\n",
    "        max_perm_dict = {}\n",
    "        for k, perms in phrase_dict.items():\n",
    "            counts = [0] * len(perms)\n",
    "            for sentence in sentences:\n",
    "                sent_str = sentence.split(\" \")\n",
    "                if set(k).issubset(set(sent_str)):\n",
    "                    indexes = sorted([sent_str.index(word) for word in k if word in sent_str])\n",
    "                    ordered_words = [sent_str[i] for i in indexes]\n",
    "                    for i, perm in enumerate(perms):\n",
    "                        if tuple(ordered_words) == perm:\n",
    "                            counts[i] += 1\n",
    "            max_count_index = counts.index(max(counts))\n",
    "            max_perm_dict[k] = perms[max_count_index]\n",
    "        return max_perm_dict\n",
    "\n",
    "    def map_transaction_tags(stemmed_sentences:List[str], frequent_features: List[Set]):\n",
    "        sentence_tags = []\n",
    "        permutation_dictionary = phrase_voting(frequent_features, stemmed_sentences)\n",
    "        # iterate over the sentences, checking if any frequent feature is a subset of the stemmed sentences\n",
    "        for i, sentence in enumerate(stemmed_sentences):\n",
    "            feature_tags = []\n",
    "            sentence_set, sentence_str = FeatureExtraction.deconstruct_sentence(sentence)\n",
    "            for feature in frequent_features:\n",
    "                if feature.issubset(sentence_set):\n",
    "                    feature = permutation_dictionary[feature]\n",
    "                    expanded_features = [stemming_dict[f] for f in feature]\n",
    "                    joined_feature = \" \".join(expanded_features)\n",
    "                    feature_tags.insert(0, joined_feature)\n",
    "            sentence_tags.append(feature_tags)\n",
    "        return sentence_tags\n",
    "    S=category_table.Sentence.to_list()\n",
    "    opinion_words, effective_opinions = get_opinion_words(sentences=S, frequent_features=combined_features)\n",
    "\n",
    "# CREATE NEW DATAFRAME WITH NEW INFOMATION \n",
    "    # Store all the processed tags in a dataframe\n",
    "    transaction_tags = database.loc[:,\"Product_ID\":\"Sentence\"].copy()\n",
    "    transaction_tags[\"Clean_Sentences\"] = category_table[\"Clean_Sentence\"]\n",
    "    transaction_tags[\"Frequent_Features\"] = map_transaction_tags(category_table[\"Clean_Sentence\"].to_list(), combined_features)\n",
    "\n",
    "    transaction_tags[\"Classes\"] = database[\"gt_score\"].apply(lambda x: 1 if x > 0 else -1 if x < 0 else 0)\n",
    "    transaction_tags[\"Opinion Words\"] = opinion_words\n",
    "    transaction_tags[\"Effective_Opinions\"] = effective_opinions\n",
    "    transaction_tags[\"Infrequent_Features\"] = [[\" \".join(x) for x in s] for s in infreq_opinions]\n",
    "    if SAVE:\n",
    "        transaction_tags.to_csv(filename)\n",
    "    \n",
    "# RETURN THE DATAFRAME\n",
    "    return transaction_tags\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline 1 - Sentiment Based of Opinion Words and Effective Opinions\n",
    "Determine Sentiment direction based off the direction of the sample paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(dataframe:pd.DataFrame, name)->pd.DataFrame:\n",
    "    validations = dataframe.Classes\n",
    "    predictions = dataframe.Sentiment_Class\n",
    "    cfm = confusion_matrix(validations, predictions)\n",
    "    conf_matrix(validations, predictions, cfm)\n",
    "    accuracy = accuracy_table(validations, predictions, name)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def sentiment_pipeline_1(dataframe: pd.DataFrame, filename)->Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    transaction_tags = dataframe\n",
    "    def determine_sentiment_on_word_polarity(set_opinion_words, set_effective_opinions):\n",
    "        # returns 1 if the polarity is positive, 0 if none, and -1 if negative\n",
    "        opinion_word_score = 0\n",
    "        effective_opinion_score = 0\n",
    "        if set_opinion_words:\n",
    "            for word in set_opinion_words:\n",
    "                opinion_word_score += determine_polarity_with_negation(word)\n",
    "        if set_effective_opinions:\n",
    "            for word in set_effective_opinions:\n",
    "                effective_opinion_score += determine_polarity_with_negation(word)\n",
    "        # if the magnitude of the effective opinion score is greater than the opinion score, return whatever the polarity is\n",
    "        if opinion_word_score == 0 and effective_opinion_score == 0:\n",
    "            return 0\n",
    "        if abs(opinion_word_score) < abs(effective_opinion_score):\n",
    "            return 1 if effective_opinion_score > 0 else -1\n",
    "        else:\n",
    "            total = opinion_word_score + effective_opinion_score\n",
    "            if total == 0:\n",
    "                return 0\n",
    "            else:\n",
    "                return 1 if total > 0 else -1\n",
    "    \n",
    "    def determine_polarity_with_negation(word):\n",
    "        # if theres no word in the set, return 0\n",
    "        if word is None:\n",
    "            return 0\n",
    "        # if the wordset contains a negation, determine the polarity of the word and invert value\n",
    "        if \"not \" in word:\n",
    "            word = word.replace(\"not\", \"\")\n",
    "            tb = TextBlob(word)\n",
    "            polarity = tb.sentiment.polarity * -1\n",
    "        else:\n",
    "            # otherwise get the polarity of the word\n",
    "            tb = TextBlob(word)\n",
    "            polarity = tb.sentiment.polarity\n",
    "        return polarity\n",
    "\n",
    "    # seperate out those sentences that are setiment bearing - they have opinion words or effective opinions that directly influence the nouns and noun phrases\n",
    "    df = transaction_tags.copy()\n",
    "    opinion_bearing_table = df[(df['Opinion Words'].apply(len) > 0) | (df['Effective_Opinions'].apply(len) > 0)]\n",
    "    op_words = opinion_bearing_table[\"Opinion Words\"].values\n",
    "    ef_op_words = opinion_bearing_table[\"Effective_Opinions\"].values\n",
    "\n",
    "    # perform the sentiment classification\n",
    "    opinion_bearing_table[\"Sentiment_Class\"] = [determine_sentiment_on_word_polarity(op_words[i], ef_op_words[i]) for i in range(len(op_words))]\n",
    "    # remove those features where no sentiment was found\n",
    "    opinion_bearing_table = opinion_bearing_table.where(opinion_bearing_table.Sentiment_Class != 0).dropna()\n",
    "    # return the opinon bearing dataframe\n",
    "    return opinion_bearing_table, evaluation(opinion_bearing_table, \"Lexicographic Direction Estimation\")\n",
    "\n",
    "\n",
    "def sentiment_pipeline_2(dataframe: pd.DataFrame, name)->Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    # remove those sentences that are not tagged as positive or negative\n",
    "    opinion_bearing_table = dataframe.copy().where(dataframe.Classes != 0).dropna()\n",
    "    X = opinion_bearing_table[\"Clean_Sentences\"].to_list()\n",
    "    y = opinion_bearing_table[\"Classes\"].to_list()\n",
    "    # perform pipeline with binary count vectorization using naive bayes and leave-one-out cross validation.\n",
    "    pipe = Pipeline([('vectorizer', CountVectorizer(binary=True)), ('naivebayes', MultinomialNB())])\n",
    "    opinion_bearing_table[\"Sentiment_Class\"] = cross_val_predict(pipe, X, y, cv=LeaveOneOut())\n",
    "    return opinion_bearing_table, evaluation(opinion_bearing_table, \"Naive Bayes\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['canon g3', 'computer', 'creative labs nomad jukebox zen xtra 40gb', 'diaper champ', 'hitachi router', 'nikon coolpix 4300', 'nokia 6600', 'nokia 6610', 'norton', 'router', 'speaker']\n",
      "\n",
      "product: CANON G3\n",
      "___________________________________________________________________________________________________\n",
      "Nouns extracted\n",
      "Performed Stemming\n",
      "Stopwords removed\n",
      "fuzzy matching done\n",
      "Stemming Dictionary created\n",
      "Frequent Features extracted\n",
      "performed feature pruning on compound phrases\n",
      "Performed psupport pruning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/42/qsqz6yz9635dllwfh9y5csxm0000gn/T/ipykernel_68094/3443854574.py:56: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  opinion_bearing_table[\"Sentiment_Class\"] = [determine_sentiment_on_word_polarity(op_words[i], ef_op_words[i]) for i in range(len(op_words))]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "`data` and `annot` must have same shape.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 42\u001b[0m\n\u001b[1;32m     40\u001b[0m         display(evaluation_table)\n\u001b[1;32m     41\u001b[0m     sentiment_db\u001b[39m.\u001b[39mto_csv(\u001b[39m\"\u001b[39m\u001b[39msentiment_classifications.csv\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 42\u001b[0m run_pipeline(pipeline\u001b[39m=\u001b[39;49msentiment_pipeline_1)\n",
      "Cell \u001b[0;32mIn[8], line 37\u001b[0m, in \u001b[0;36mrun_pipeline\u001b[0;34m(n, pipeline)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mproduct: \u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m.\u001b[39mupper()\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m___________________________________________________________________________________________________\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     36\u001b[0m feature_database \u001b[39m=\u001b[39m feature_extraction_table(database, name)\n\u001b[0;32m---> 37\u001b[0m opinion_table, evaluation_table \u001b[39m=\u001b[39m pipeline(feature_database, name)\n\u001b[1;32m     38\u001b[0m sentiment_db \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat([sentiment_db, opinion_table])\n\u001b[1;32m     39\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mEVALUATION TABLE:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 60\u001b[0m, in \u001b[0;36msentiment_pipeline_1\u001b[0;34m(dataframe, filename)\u001b[0m\n\u001b[1;32m     58\u001b[0m opinion_bearing_table \u001b[39m=\u001b[39m opinion_bearing_table\u001b[39m.\u001b[39mwhere(opinion_bearing_table\u001b[39m.\u001b[39mSentiment_Class \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mdropna()\n\u001b[1;32m     59\u001b[0m \u001b[39m# return the opinon bearing dataframe\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m \u001b[39mreturn\u001b[39;00m opinion_bearing_table, evaluation(opinion_bearing_table, \u001b[39m\"\u001b[39;49m\u001b[39mLexicographic Direction Estimation\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[7], line 5\u001b[0m, in \u001b[0;36mevaluation\u001b[0;34m(dataframe, name)\u001b[0m\n\u001b[1;32m      3\u001b[0m predictions \u001b[39m=\u001b[39m dataframe\u001b[39m.\u001b[39mSentiment_Class\n\u001b[1;32m      4\u001b[0m cfm \u001b[39m=\u001b[39m confusion_matrix(validations, predictions)\n\u001b[0;32m----> 5\u001b[0m conf_matrix(validations, predictions, cfm)\n\u001b[1;32m      6\u001b[0m accuracy \u001b[39m=\u001b[39m accuracy_table(validations, predictions, name)\n\u001b[1;32m      7\u001b[0m \u001b[39mreturn\u001b[39;00m accuracy\n",
      "File \u001b[0;32m~/Dropbox/Studies/Bath_Artificial_Intelligence/Course Material/6_NLP/Programming/Submission/opinionmining.py:370\u001b[0m, in \u001b[0;36mconf_matrix\u001b[0;34m(validations, predictions, cfm)\u001b[0m\n\u001b[1;32m    367\u001b[0m confusion_matrix \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mcrosstab(validations, predictions, rownames\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mActual\u001b[39m\u001b[39m'\u001b[39m], colnames\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mPredicted\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m    369\u001b[0m \u001b[39mwith\u001b[39;00m plt\u001b[39m.\u001b[39mstyle\u001b[39m.\u001b[39mcontext({\u001b[39m'\u001b[39m\u001b[39mfigure.facecolor\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mwhite\u001b[39m\u001b[39m'\u001b[39m}): \n\u001b[0;32m--> 370\u001b[0m     sns\u001b[39m.\u001b[39;49mheatmap(confusion_matrix, annot\u001b[39m=\u001b[39;49mlabels, fmt\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m, cmap\u001b[39m=\u001b[39;49msns\u001b[39m.\u001b[39;49mcubehelix_palette(start\u001b[39m=\u001b[39;49m\u001b[39m1.5\u001b[39;49m, rot\u001b[39m=\u001b[39;49m\u001b[39m0.4\u001b[39;49m, dark\u001b[39m=\u001b[39;49m\u001b[39m.5\u001b[39;49m, light\u001b[39m=\u001b[39;49m\u001b[39m.75\u001b[39;49m,reverse\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, as_cmap\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m))\n\u001b[1;32m    371\u001b[0m     plt\u001b[39m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nlp_env/lib/python3.9/site-packages/seaborn/matrix.py:446\u001b[0m, in \u001b[0;36mheatmap\u001b[0;34m(data, vmin, vmax, cmap, center, robust, annot, fmt, annot_kws, linewidths, linecolor, cbar, cbar_kws, cbar_ax, square, xticklabels, yticklabels, mask, ax, **kwargs)\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Plot rectangular data as a color-encoded matrix.\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \n\u001b[1;32m    367\u001b[0m \u001b[39mThis is an Axes-level function and will draw the heatmap into the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    443\u001b[0m \n\u001b[1;32m    444\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    445\u001b[0m \u001b[39m# Initialize the plotter object\u001b[39;00m\n\u001b[0;32m--> 446\u001b[0m plotter \u001b[39m=\u001b[39m _HeatMapper(data, vmin, vmax, cmap, center, robust, annot, fmt,\n\u001b[1;32m    447\u001b[0m                       annot_kws, cbar, cbar_kws, xticklabels,\n\u001b[1;32m    448\u001b[0m                       yticklabels, mask)\n\u001b[1;32m    450\u001b[0m \u001b[39m# Add the pcolormesh kwargs here\u001b[39;00m\n\u001b[1;32m    451\u001b[0m kwargs[\u001b[39m\"\u001b[39m\u001b[39mlinewidths\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m linewidths\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nlp_env/lib/python3.9/site-packages/seaborn/matrix.py:177\u001b[0m, in \u001b[0;36m_HeatMapper.__init__\u001b[0;34m(self, data, vmin, vmax, cmap, center, robust, annot, fmt, annot_kws, cbar, cbar_kws, xticklabels, yticklabels, mask)\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[39mif\u001b[39;00m annot_data\u001b[39m.\u001b[39mshape \u001b[39m!=\u001b[39m plot_data\u001b[39m.\u001b[39mshape:\n\u001b[1;32m    176\u001b[0m             err \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m`data` and `annot` must have same shape.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 177\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(err)\n\u001b[1;32m    178\u001b[0m     annot \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \u001b[39m# Save other attributes to the object\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: `data` and `annot` must have same shape."
     ]
    }
   ],
   "source": [
    "def get_paths():\n",
    "    paths = []\n",
    "    for folder in os.listdir(full_path):\n",
    "        try:\n",
    "            for file in os.listdir(os.path.join(full_path, folder)):\n",
    "                if str(file) != \"Readme.txt\" and str(file) != \".DS_Store\":\n",
    "                    paths.append(os.path.join(full_path, folder, file))\n",
    "        except NotADirectoryError:\n",
    "            continue\n",
    "    return paths\n",
    "\n",
    "\n",
    "def write_database(paths):\n",
    "    pwd = os.getcwd()\n",
    "    filename = \"db.csv\"\n",
    "    if SAVE:\n",
    "        db = ReviewDatabase(paths).dataframe.to_csv(os.path.join(pwd, filename))\n",
    "        db = pd.read_csv(filename)\n",
    "    else:\n",
    "        db = pd.read_csv(filename)\n",
    "    return db\n",
    "        \n",
    "def get_product_database(db, product_name):\n",
    "    return db.where(db[\"Product_name\"] == product_name).copy().dropna()\n",
    "\n",
    "def run_pipeline(n = 10, pipeline=sentiment_pipeline_2):\n",
    "    paths = get_paths()\n",
    "    db = write_database(paths)\n",
    "    sentiment_db = pd.DataFrame()\n",
    "    product_names = sorted(set(db[\"Product_name\"]))\n",
    "    print(product_names)\n",
    "    for i in range(0, n):\n",
    "        name = product_names[i]\n",
    "        database = get_product_database(db=db, product_name=name)\n",
    "        print(f\"\\nproduct: {name.upper()}\\n___________________________________________________________________________________________________\")\n",
    "        feature_database = feature_extraction_table(database, name)\n",
    "        opinion_table, evaluation_table = pipeline(feature_database, name)\n",
    "        sentiment_db = pd.concat([sentiment_db, opinion_table])\n",
    "        print(\"\\nEVALUATION TABLE:\\n\")\n",
    "        display(evaluation_table)\n",
    "    sentiment_db.to_csv(\"sentiment_classifications.csv\")\n",
    "run_pipeline(pipeline=sentiment_pipeline_1)\n",
    "\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
