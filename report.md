
# Opinon Miner  
  
## Step 1: Task and Data Analyses  
Typically, sentiment analysis involves determining whether any given document contains positive or negative sentiment. Determining the direction of sentiment can be useful in many applications where it might be valuable to understand the performance  of some offering, in the broadest of respects. Whether for a company, product, service, automatic reporting of sentiment allows a quick way to gauge the how the device performs and saves time for the reader. However, going further than reporting pure positive or negative sentiment, it might sometimes be more valuable to understand exactly which components of a given report, review or document might lead to an overall feeling of sentiment, and therefore a more in-depth analysis to the core strengths and weaknesses of a given offering is required. 

Undertaking such a task is called opinion mining and this is the basis of this project. The task of this project is to perform opinion mining on a set of product reviews given in the data folder which are taken from Amazon.com. The project will aim to replicate the opinion mining procedure done on the same set of reviews prepared by (Hu and Liu, 2004) whose sentiment classification was performed using lexicographical approach and compare the performance to a machine learning approach using a suitable algorithm. Finally, the aim is to mine frequent features from each product so that they can be reported with a summary of their corresponding sentiment that is either positive or negative. For each feature, we report the $n_{features}$  most commonly occurring feature  with  individual totals of positive and negative reviews, along with some $n_{reviews}$ , user-determined number of example reviews of that each positive and negative class. 

Through reviewing the product reviews, its clear that a comprehensive pipeline is required to transform the unstructured data in the review sets into structured opinion sets reporting mined features. For each product there is a set of reviews that have split into sentences with corresponding positive and negative review scores, if the user provided a score. With this in mind, an algorithm that uses regular expressions to match all the sentences and their scores is required and the process of simply extracting each sentence and its review score to create a database is an important step in initialising the data preprocessing. This will be discussed more in the next section. The quantity of the number of reviews varies between a few thousand and a few hundred. This is an important note as it steers which types of machine learning models are suitable to this task. Doing so will require a few steps including data preprocessing which first involves separating each set of reviews into sentences, where there are inconsistencies between the review sets, tokenisation, part-of-speech tagging, fuzzy-matching, and stemming. feature extraction for most commonly occurring features and the pruning irrelevant features, sentiment analysis using both the lexicographic approach outlined in (Hu and Liu, 2004) as well as a machine learning, performance evaluation and finally, summarisation and reporting. The outcome of the performance evaluation phrase will determine the final algorithm with which the mined sentiments are reported.



## Step 2: Data Preprocessing  

The relation between each step in the methodology is shown in *diagram 1*. There are two major steps to this process - the first is segmenting each sentence and its ground truth score from the inconsistent data sets. To do so, a number of regular expressions and error handling techniques are used to handle different cases. For example, removing title annotations, review tags delimited by `[t]`, extracting each sentence limited by `##` and then extracting the ground truth scores like `[-1]`. Regular expressions make use of deterministic finite automata to search a corpus of text to match strings in a given vocabulary (Jurafsky and Martin, 2023, p.5). It is necessary to extract these review scores as they provide a ground truth with which performance can be evaluated. Sentence segmentation is appropriate since the task is to provide feedback on frequently occurring features and typically commenting on features happens at the sentence level as opposed to the paragraph level.  For example "*I use Norton 2003 at work with absolutely no trouble.*" and "*I've heard Norton 2004 version is fine too*" are part of the same review paragraph but discuss two features independently of each other. The combinations of products, sentences and their respective scores is the foundational database which can facilitate further data preprocessing. 

If the aim is to eventually perform sentiment analysis using a machine learning model then one of the targets for data preprocessing is to first normalise the text. A text that is normalised is a text that is in a compact form by means of reducing the size and complexity of the text. Doing so is advantageous because it reduces the dimensionality of the vocabulary that describes "bag-of-words" vectors. This is also useful in frequent feature extraction because constraining the search space for semantically similar words will group them together and allow more efficient extraction of features. Working backwards, the aim is to process the data into a form that is as semantically compact as possible by incorporating stemming. Stemming is the process of reducing different forms of the same word (and sometimes other words) to a more simple versions of itself. By doing so, stemming can make vocabularies of bag-of-word type vectors more compact and reduce vector sparsity which is desirable in  machine learning models like Naive Bayes and Logistic Regression. (Wang and Manning, 2012). 
### Tokenisation
To perform stemming it is necessary to first break up sentences into their constituent words by applying tokenisation. The process of splitting tokens from sentences is closely related to named-entity-recognition algorithms. This is because words cannot simply be split on whitespaces, punctuation and clitics because these are common features in meaningful compound words and objects like website addresses and place names. (Jurafsky and Martin, 2008, p.5). Following tokenisation, part of speech tagging is performed before stemming to preserve the semantic meaning for the POS tagger. 
### Part of Speech Tagging
To identify and extract the noun phrases part-of-speech tagging is used from the `spacy` library. There are two types of part of speech tagging: rule based part for speech tagging and Hidden Markov Model tagging that use statistical models. 
#### CFG tagging
Rules- based POS tagging uses a context free grammar to create a derivation of a parse tree where each terminal symbol is a part of speech is in the lexicon of the grammar corresponding to a word in the sentence. The non-terminal symbols of these CFGs that are particularly relevant to this task, especially where noun extraction is concerned, are those of noun phrases which usually follow the rule `NP->Nominal, NP->Det`, where in its simplest form `Nominal->Noun`.  This tells us that at its simplest a noun phrase is a determinant and noun. For this reason, we extract only the nouns, rather than the noun chunks because the determinants will be culled by removing the stop words. 

#### HMM tagging
Since we are using `spacy`'s POS tagger, it is more likely tagging is done using a Hidden Markov Model (HMM). HMM taggers use bayesian inference to perform sequence classification tasks that allow for determining the parts of speech tags given a sequence of words. The equation below taken from (Jurafsky and Martin, 2008, p.140) describes the essence of the mathematics behind part of speech tagging with Hidden Markov models.
$$
\hat t_{1:n} = \text{argmax}_{t_{1:n}}P(t_{1}...t_{n}| w_{1}...w_{n})  	
$$
The intuition behind this equation is to choose the  tag sequence from all the possible combinations of tags $t_{1:n}$, word sequence $w_{1:n}$ of a fixed length $n$ the set of tags which maximise the probability observing a set of tags, given some sequence of words. What it is saying is that the word sequence has been observed, the tag sequence that maximises the posterior probability of what has been observed is most likely to be the correct tag sequence, and should therefore be picked. This equation can be shown to be expressed using the HMM emission and transition matrices, approximately, by the following equation from (Kochmar,  2023):
$$
\approx \underset{t_{1} \ldots t_{n}}{\operatorname{argmax}} \prod_{i=1}^{n} \overbrace{P\left(w_{i} \mid t_{i}\right)}^{\text
{emission }} \overbrace{P\left(t_{i} \mid t_{i-1}\right)}^{\text {transition }}$$
Here, the emission matrix is the set of observable features (the words) and the hidden features (tags) are the transition matrix. The equation is asking to find the combination of hidden states (emission) that maximises the observable states (transition). In other words, find the set of tags that maximise the chances of observing this sequence of words. Since this equation has time complexity of $O(n^{t})$, the *Viterbi* algorithm is commonly used to improve this efficiency. Using dynamic programming, the *Viterbi* algorithm prunes states that cannot yield maximum probabilities thereby significantly reducing the number of combinations needed to check for a correct tag sequence. 

### Minimum Edit Distance
After performing part of speech tagging, the nouns from each sentence are extracted and saved into a new column in the database. We then perform fuzzy matching on the nouns. This is done by splitting the list into two halves where the bottom half is matched with the top half. Using Zipf's law to guide this heuristic, it is unlikely that a word is misspelled frequently enough for it to appear in the top half of the word distribution. To perform fuzzy matching we use `nltk`'s minimum edit distance. The minimum edit distance measures the number of insertions, deletions, and substitutions required to change one string into another (Jurafsky and Martin, 2008, p.75). We use a maximum minimum edit distance of 2. Following fuzzy matching, the nouns and sentences are stemmed. In this case, we use the `nltk`'s  `SnowballStemmer` which is an improved modification on the `PorterStemmer`. 
### Stemming
The Porter stemmer (Porter, 1980) is a stemming algorithm that transforms words into base forms of consonants and verbs by removes suffixes through a 5-step series of rewrite rules.  The first step performs rules on a string that rewrites plurality, past participles, the following four steps removes and adds features to the string following linguistic morphological patterns to reach a final form of countable sequences of continents and vowels. *Diagram 2* shows how the algorithm stems an arbitrarily chosen word "predicated" to "pred". 

![[porter_stemmer.jpg]]
*Diagram 2: Porter stemmer performing stemming on "predicate". Words at bottom of each step move to top of following step. Stemmer rules denoted in blue.*

Finally, after stemming is complete, a dictionary is created to map each stemmed word into an expanded word. The expanded word will be used later in the process when it is required to re-present the nouns as labelled features in the summary document. The dictionary maps each stem to the most frequent word that is reduced to that stem. 

After tokenization, part of speech tagging, fuzzymatching, stemming the tokens from the sentences and nouns extraction, the data preprocessing is complete and product feature extraction can begin.
## Step 3: Product Feature Extraction  
The product feature extraction pipeline closely follows the algorithms described in (Hu and Liu, 2004) with a few modifications. The purpose of this phase is to improve the set of extracted nouns by pairing them with nouns and adjectives so that the product feature titles can be more comprehensive and accurate than a set of nouns. 
### Frequent Feature Selection
To perform frequent selection an association rule mining technique using the apriori algorithm is used, from the `mlxtend` library. The purpose is to identify the noun tuples that appear often enough to be useful. Let $N$ be a set of nouns $n_1, n_2 ...$ where $max(|N|)=3$.  The apriori algorithm happens in two steps calculating relies on two key metrics: support and confidence.  For nouns $n_{1}, n_{2}$:
$$
\text{Support}(n_{1}) = \frac{\text{number of sets containing } n_{1}}{\text{total number of sets}}
$$
For support, a minimum threshold of 1% is used. The confidence of each set measures the strength of association for the elements in set $N = \set{n_{1}, n_{2}}$ , or mathematically:
$$
\text{Confidence}(n_{1}\rightarrow n_{2}) = \frac{\text{Support}(n_{1}\cup n_{{2}})}{\text{Support}(n_{1})}
$$
In other words, we can use these two measure to extract and prune features where the frequency of combinations of nouns that appear above a certain threshold (the support), and then use the confidence to prune those that combinations of pairs that are not strongly associated enough. We use a confidence of 60%. This allows us to create feature sets like `{{"internet","norton","security"}, {"internet", "security"}, {"norton"}}`. This is desirable if we want to extract features that are not just single words.  
### Feature Pruning 

## Step 4: Sentiment Analysis  
  
## Step 5: Evaluation and discussion

## Bibliography

